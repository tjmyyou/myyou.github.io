<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
    <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" href="jemdoc.css" type="text/css" />
    <link rel="stylesheet" href="tju.css" type="text/css" />
    <title>Selected Publications</title>
</head>
<body>
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?f85ae40b6d54f8f7becb3b0be41d4515";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
    <table summary="Table for page layout." id="tlayout">
        <tr valign="top">
            <td id="layout-menu">
                <div class="image-container">
                    <img src="./projects/Tongji_University_Emblem.svg.png" width="96px" height="96px" alt="TJU">
                </div>
                <div class="menu-item"><a href="index.html">Homepage</a></div>
                <div class="menu-item"><a href="pub.html" class="current">Publications</a></div>
                <div class="menu-item"><a href="group.html">Members</a></div>
                <div class="menu-item"><a href="service.html">Services</a></div>
                <div class="menu-item"><a href="award.html">Awards</a></div>
            </td>
            <td id="layout-content">
                <div id="toptitle">
                    <h1>Selected Pulications </h1>
                </div>
                <p style="text-align:justify"><strong><font size="4px">Publications</font></strong> (* equal contribution, <b><span style="font-family:Wingdings">*</span></b> corresponding author. All publications on [<a href="https://scholar.google.com/citations?user=cZ-SxbkAAAAJ&hl=zh-CN"
                        target=&ldquo;blank&rdquo;>Google Scholar</a>])
        <table id="pubList" border="0" cellpadding="0" width="100%" style="border-spacing: 10 18px; line-height:16pt; border: 0px;">
        <!-- 01 -->
		<tr>
            <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://ieeexplore.ieee.org/abstract/document/10752365">
					<papertitle><b>Closely Cooperative Multi-Agent Reinforcement Learning Based on Intention Sharing and Credit Assignment</b></papertitle>
				</a>
				<br> Hao Fu, <b>Mingyu You*</b>, Hongjun Zhou, Bin He
				<br>
				<em>IEEE Robotics and Automation Letters</em>, 2024
				<br>
				<a href="https://ieeexplore.ieee.org/abstract/document/10752365">paper</a>/<a href="videos/01_fh.mp4">video</a>
				<br>
				<b>Abstract </b>Collaborative tasks are important in multi-agent systems. Multi-agent reinforcement learning is a commonly used technique for solving multi-agent cooperative policy learning. The closely collaborative task is a special but common case within cooperative tasks, where the change in the environmental state requires multiple agents to simultaneously perform specific actions. For example, in a box-pushing task where the boxes are heavy and require multiple agents to push simultaneously. The closely cooperative task faces some unique challenges. Firstly, the completion of a closely collaborative task requires agents to synchronize their actions, necessitating a consistent intention among them. Secondly, when some agents' erroneous actions lead to task failure, it becomes a challenge to avoid incorrectly penalizing agents who performed the correct actions. These challenges make most of the existing MARL methods perform poorly on this task. In this paper, we propose a closely collaborative multi-agent reinforcement learning(CC-MARL) algorithm based on intention sharing and credit assignment. We use a two-phase training to learn intention encoding and intention sharing respectively, and decompose joint action values based on counterfactual baseline ideas. We deployed scenarios in both simulated and real environments with various sizes, numbers of boxes, and numbers of agents and compare CC-MARL with various classical MARL algorithms on box-pushing tasks of different map scales in simulation, demonstrating the state-of-the-art of our method.
				<p></p>
                <br>
                <div style="text-align: center;">
                    <img src="./projects/01_fh.JPG" width=70%>
                </div>
            </td>
        </tr>
		<!-- 02 -->
        <tr>
            <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://dl.acm.org/doi/full/10.1145/3702226">
					<papertitle><b>Physical-aware 3D Shape Understanding for Finishing Incomplete Assembly</b></papertitle>
				</a>
				<br> Weihao Wang, <b>Mingyu You*</b>, Hongjun Zhou, Bin He
				<br>
				<em>ACM Transactions on Graphics</em>, 2024
				<br>
				<a href="https://dl.acm.org/doi/full/10.1145/3702226">paper</a>/<a href="videos/02_wwh.mp4">video</a>
				<br>
				<br>
				<b>Abstract </b>Understanding the part composition and structure of 3D shapes is crucial for a wide range of 3D applications, including 3D part assembly and 3D assembly completion. Compared to 3D part assembly, 3D assembly completion is more complicated, which involves repairing broken or incomplete furniture that miss several parts with a toolkit. Given an incomplete assembly, 3D assembly completion seeks to identify its missing parts from multiple candidates, determine their poses, and produce complete assembly that is well-connected, structurally stable, and aesthetically pleasing. This task necessitates not only specialized knowledge of part composition but, more importantly, an awareness of physical constraints, i.e., connectivity, stability, and symmetry. Neglecting these constraints often results in assemblies that, although visually plausible, are impractical. To address this challenge, we propose PhysFiT, a physical-aware 3D shape understanding framework. This framework is built upon attention-based part relation modeling and incorporates connection modeling, simulation-free stability optimization and symmetric transformation consistency. We evaluate its efficacy on 3D part assembly and 3D assembly completion, a novel assembly task presented in this work. Extensive experiments demonstrate the effectiveness of PhysFiT in constructing geometrically sound and physically compliant assemblies.
				<p></p>
                <br>
                <div style="text-align: center;">
                    <img src="./projects/02_wwh.jpg" width=70%>
                </div>
            </td>
        </tr>
		<!-- 03 -->
        <tr>
            <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681678">
					<papertitle><b>Scene Diffusion: Text-driven Scene Image Synthesis Conditioning on a Single 3D Model</b></papertitle>
				</a>
				<br> Xuan Han,Yihao Zhao,<b>Mingyu You*</b>
				<br>
				<em>ACM MM oral presentation (3.87%)</em>, 2024
				<br>
				<a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681678">paper</a>/<a href="videos/03_hx.mp4">video</a>
				<br>
				<br>
				<b>Abstract </b>Scene image is one of the important windows for showcasing product design. To obtain it, the standard 3D-based pipeline requires designer to not only create the 3D model of product, but also manually construct the entire scene in software, which hindering its adaptability in situations requiring rapid evaluation. This study aims to realize a novel conditional synthesis method to create the scene image based on a single-model rendering of the desired object and the scene description. In this task, the major challenges are ensuring the strict appearance fidelity of drawn object and the overall visual harmony of synthesized image. The former's achievement relies on maintaining an appropriate condition-output constraint, while the latter necessitates a well-balanced generation process for all regions of image. In this work, we propose Scene Diffusion framework to meet these challenges. Its first progress is introducing the Shading Adaptive Condition Alignment (SACA), which functions as an intensive training objective to promote the appearance consistency between condition and output image without hindering the network's learning to the global shading coherence. Afterwards, a novel low-to-high Frequency Progression Training Schedule (FPTS) is utilized to maintain the visual harmony of entire image by moderating the growth of high-frequency signals in the object area. Extensive qualitative and quantitative results are presented to support the advantages of the proposed method. In addition, we also demonstrate the broader uses of Scene Diffusion, such as its incorporation with ControlNet.
				<p></p>
                <br>
                <div style="text-align: center;">
                    <img src="./projects/03_hx.JPG" width=70%>
                </div>
            </td>
        </tr>
		<!-- 04 -->
		<tr>
		    <td style="padding:10px;width:100%;vertical-align:top;text-align:justify;">
				<a href="https://arxiv.org/pdf/2410.08192">
					<papertitle><b>HybridBooth: Hybrid Prompt Inversion for Efficient Subject-Driven Generation</b></papertitle>
				</a>
				<br>Shanyan Guan, Yanhao Ge, Yin Tan, Jian Yang, Wei Li,<b>Mingyu You*</b>
				<br>
				<em>ECCV</em>, 2024
				<br>
				<a href="https://arxiv.org/pdf/2410.08192">paper</a>
				<br>
				<br>
				<b>Abstract</b> Recent advancements in text-to-image diffusion models have shown remarkable creative capabilities with textual prompts, but generating personalized instances based on specific subjects, known as subjectdriven generation, remains challenging. To tackle this issue, we present a new hybrid framework called HybridBooth, which merges the benefits of optimization-based and direct-regression methods. HybridBooth operates in two stages: the Word Embedding Probe, which generates a robust initial word embedding using a fine-tuned encoder, and the Word Embedding Refinement, which further adapts the encoder to specific subject images by optimizing key parameters. This approach allows for effective and fast inversion of visual concepts into textual embedding, even from a single image, while maintaining the modelâ€™s generalization capabilities.
				<p></p>
		        <br>
		        <div style="text-align: center;">
		            <img src="./projects/04_gyh.JPG" width=70%>
		        </div>
		    </td>
		</tr>

